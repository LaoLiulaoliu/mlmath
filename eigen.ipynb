{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵和行列式的几何意义\n",
    "\n",
    "矩阵的初等行变换（相当于给矩阵左乘矩阵$A$）在几何空间的变化，伸缩或旋转。\n",
    "\n",
    "矩阵$A$ 的行列式的几何意义是线性变换$A$ 下的图形面积或体积的伸缩因子。\n",
    "\n",
    "1. 矩阵初等变换，用非零常数乘以矩阵某一行。\n",
    "\n",
    "一个向量 $X$ 左乘矩阵 $A$：\n",
    "$$\n",
    "A \\cdot X =\n",
    "  \\begin{bmatrix}\n",
    "  3 & 0\\\\\n",
    "  0 & 0.5\\\\\n",
    "  \\end{bmatrix}\n",
    "\\cdot\n",
    "  \\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix}\n",
    "=\n",
    "  \\begin{bmatrix} 3x\\\\ 0.5y\\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$A$ 为对角阵时，主对角线上每一个元素，对一个维度进行拉伸变换，$x$ 拉伸为3倍，$y$ 缩小为0.5倍。\n",
    "\n",
    "经过变换，$x$ 和$y$ 张成矩形面积增加1.5倍，矩阵的行列式$|A| = 1.5$。\n",
    "\n",
    "2. 矩阵初等变换，互换矩阵的两行。\n",
    "\n",
    "$$\n",
    "A \\cdot X =\n",
    "  \\begin{bmatrix}\n",
    "  0 & 1\\\\\n",
    "  1 & 0\\\\\n",
    "  \\end{bmatrix}\n",
    "\\cdot\n",
    "  \\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix}\n",
    "=\n",
    "  \\begin{bmatrix} y\\\\ x\\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "沿着$y$ = $x$ 做了一个对称旋转，高维中，是其中两维组成的对称轴做对称旋转。\n",
    "\n",
    "经过变换，$x$ 和$y$ 张成矩形面积增加-1倍，矩阵的行列式$|A| = -1$。\n",
    "\n",
    "3. 矩阵初等变换，第i行k倍加到第j行。\n",
    "$$\n",
    "A \\cdot X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2\\\\\n",
    "  0 & 1\\\\\n",
    "  \\end{bmatrix}\n",
    "\\cdot\n",
    "  \\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix}\n",
    "=\n",
    "  \\begin{bmatrix} x+2y\\\\ y\\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "第二行的两倍加到第一行，$y$轴沿竖直方向大小不变，$x$轴沿水平方向上左右拉伸，$A_{12}$的为正，向右拉伸。\n",
    "\n",
    "经过变换，$x$ 和$y$ 张成矩形面积不变，矩阵的行列式$|A| = 1$。\n",
    "\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) 上一些线性变换的展示：\n",
    "\n",
    "![](./img/eigenvalues_transformations.png)\n",
    "\n",
    "矩阵的初等行变换中，左乘矩阵的所有列向量组成一组新基，原矩阵$X$ 是在老基视角下的数值表示，变换后矩阵$Y$ 是在新基视角下的数值表示，左乘的矩阵$A$是过渡矩阵的逆$C^{-1}$，$Y =C^{-1} X$。如果老基组成的矩阵是单位矩阵$I$，$I \\cdot C = C$，那么过渡矩阵$C$ 就是新基。变换2的矩阵$A$是正交矩阵（$A \\cdot A^T = I$），所以这个变换叫正交变换，变换前后向量长度，向量间内积不变，正交变换只是对向量用另一组标准正交基表示，向量往新基的各个方向做变换，老基是，\n",
    "$$\n",
    "(e_1, e_2) =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0\\\\\n",
    "  0 & 1\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "经过正交变换，矩阵的新基，\n",
    "$$\n",
    "(\\epsilon_1, \\epsilon_2) =\n",
    "  \\begin{bmatrix}\n",
    "  0 & 1\\\\\n",
    "  1 & 0\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "相似矩阵是一个线性变换在不同基下的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征值分解（EVD）\n",
    "\n",
    "对n阶方阵$A$，如果存在一个数$\\lambda$和一个n维非零列向量$v$，使得$A \\cdot v = \\lambda \\cdot v$，则$\\lambda$是矩阵$A$的一个特征值，$v$是特征值$\\lambda$下的特征向量。\n",
    "\n",
    "方阵$A$的特征值、特征向量的几何意义，对特征向量$v$做初等行变换（左乘$A$），等于对特征向量$v$伸缩$\\lambda$倍。特征值表示伸缩系数，特征向量是只有伸缩变换，没有旋转变换的线性变换中的方向向量。\n",
    "\n",
    "线性代数中，方阵$A$可被相似对角化，当且仅当$A$有$n$个线性无关的特征向量，相似对角矩阵$\\Lambda = P^{-1} A P$，推导出$A = P \\Lambda P^{-1}$。$n$个线性无关的特征向量，构成相似变换矩阵$P$，$P$可逆。$P$ 对$A$ 做相似变换。\n",
    "\n",
    "$n$个特征值构成相似对角阵$\\Lambda$的主对角线元素，若$n$个特征值不同，则对应的$n$个特征向量线性无关，与$A$可相似对角化互为充要条件。若任意特征值$\\lambda_i$有$k$重根，则$(A - \\lambda_i I) x = 0$所对应的基础解系解向量的个数也是$k$，也即特征值$\\lambda_i$对应$k$个线性无关的特征向量（总共保证$n$个线性无关特征向量），与$A$可相似对角化互为充要条件，其中$k = n - r(A - \\lambda_i I)$。\n",
    "\n",
    "特征值分解（Eigen value decomposition）就是把矩阵用相似对角阵表示，$A = Q \\Sigma Q^{-1} = P \\Lambda P^{-1}$。\n",
    "\n",
    "对角矩阵$\\Sigma$ 的主对角线元素是特征值，特征值从大到小排列，特征值所对应的线性无关特征向量，在Q中以列向量的形式从左到右排列。\n",
    "在高维空间中，矩阵$A$表示一个线性变换，$n$个特征向量表示矩阵最重要的$n$个变换方向，用这$n$个变换方向可以近似矩阵（变换），特征值表示这个特征的重要程度。\n",
    "\n",
    "如果$A$是实对称矩阵，则$n$个特征向量两两正交。$Q$ 对$A$ 做正交变换，同时也是相似变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 奇异值分解（SVD）\n",
    "奇异值分解（Singular value decomposition）比特征值分解更通用，不要求所变换矩阵是方阵。\n",
    "$$\n",
    "A_{m x n} = U_{m x m} \\cdot \\Sigma_{m x n} \\cdot V^{T}_{n x n}\n",
    "$$\n",
    "\n",
    "$U$ 和 $V$ 都是正交矩阵，$U$ 是左奇异矩阵，$V$ 是右奇异矩阵，$\\Sigma$ 仅在主对角线上有值，称为奇异值，其他元素为零。\n",
    "\n",
    "如何做奇异值分解？\n",
    "\n",
    "$(A A^T)^T = A A^T$，$A A^T$ 在实数域是$m x m$ 实对称矩阵，可以用特征值分解，$(A A^T) u_i = \\lambda_i u_i$，得到矩阵$A A^T$ 的$m$个特征值和对应的$m$个两两正交的特征向量。$m$个特征向量组成左奇异矩阵$U$，每个特征向量叫做左奇异向量。\n",
    "\n",
    "同理，$(A^T A) v_i = \\lambda_i v_i$，得到$n$个特征值，对应的$n$个两两正交特征向量组成右奇异矩阵，每个向量叫右奇异向量。\n",
    "\n",
    "$U$ 和$V$ 是单位正交阵，根据正交矩阵的定义，$U^T = U^{-1}, V^T = V^{-1}$。\n",
    "\n",
    "$A = U \\Sigma V^T$，两边右乘$V$，$A V = U \\Sigma V^{-1} V = U \\Sigma$。这个矩阵乘法可以把每一列拆出来相乘，结果列做concatenate 等于原结果（分块矩阵乘法），于是有$A v_i = u_i \\sigma_i$，所以奇异值$\\sigma_i = A v_i / u_i$。\n",
    "\n",
    "至此，奇异值分解的三个分量都求出。（But why this way can decompose singular value？）\n",
    "\n",
    "知道奇异值分解公式后，推导奇异值和特征值的关系。\n",
    "$$\n",
    "(A A^T) = (U \\Sigma V^T) \\cdot (V \\Sigma^T U^T) = U \\Sigma \\Sigma^T U^T \\\\\n",
    "(A^T A) = (V \\Sigma^T U^T) \\cdot (U \\Sigma V^T) = V \\Sigma^T \\Sigma V^T\n",
    "$$\n",
    "\n",
    "$\\Sigma \\Sigma^T$ 是$m x m$，$\\Sigma^T \\Sigma$ 是$n x n$，但他们主对角线上的奇异值都相等，是$\\Sigma$对角线奇异值的平方。若$m < n$，则$\\Sigma$ 主对角线上元素个数$< m$。所以$\\Sigma \\Sigma^T = \\Sigma^2_{m x m}$，$\\Sigma^T \\Sigma = \\Sigma^2_{n x n}$。\n",
    "\n",
    "$$\n",
    "(A A^T) = U \\Sigma^2 U^T = U \\Sigma^2 U^{-1} \\\\\n",
    "(A^T A) = V \\Sigma^2 V^T = V \\Sigma^2 V^{-1}\n",
    "$$\n",
    "\n",
    "根据相似对角化公式，$(A A^T)$ 和$(A^T A)$ 的特征值是矩阵A奇异值的平方，$\\lambda_i = \\sigma^2_i$，奇异值等于特征值的平方根，$\\sigma_i = \\sqrt{\\lambda_i}$。\n",
    "\n",
    "因此，奇异值在矩阵$\\Sigma$ 主对角线上也是从大到小排列，奇异值的平方--特征值在矩阵$\\Sigma^2$ 上也从大到小排列，特征值所对应的特征向量，分别在$U$，$V$中以列向量的形式从左到右排列。\n",
    "\n",
    "奇异值分解中，大部分情况下，前10%甚至1%的奇异值之和，就占了所有奇异值总和的99%以上，可以用前k个奇异值近似描述矩阵。\n",
    "\n",
    "$$\n",
    "A_{m x n} = U_{m x m} \\cdot \\Sigma_{m x n} \\cdot V^{T}_{n x n} \\approx U_{m x k} \\cdot \\Sigma_{k x k} \\cdot V^T_{k x n} \\quad  (k << n)\n",
    "$$\n",
    "\n",
    "奇异值分解成三个小矩阵相乘，近似原矩阵，达到降维目的，减少了空间存储，可以用来做有损压缩。奇异值和特征值表示数据信息量，数值越大，数据信息量越大。\n",
    "\n",
    "SVD计算举例，可以看这篇文章 [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)--第三节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] [【线性代数的几何意义】行列式的几何意义](https://www.cnblogs.com/AndyJee/p/3491487.html)\n",
    "\n",
    "[2] [机器学习中SVD总结](https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg)\n",
    "\n",
    "[3] [SVD（奇异值分解）Python实现](https://www.cnblogs.com/endlesscoding/p/10058532.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
