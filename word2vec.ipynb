{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vector\n",
    "\n",
    "Word2vec (word embedding)词用向量表示。分Skip-gram 和CBOW 两种训练方法。数据量大时，Skip-gram效果好。\n",
    "Skip-gram 基于中心词生成背景词，CBOW 基于背景词生成中心词。\n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "Skip-gram 思路是相邻越近的两个词的相似度越高。越远的相似度越低。\n",
    "\n",
    "- 近义词：两个词是近义词，则这两个词分别邻近的词会很相似，所以近义词的词向量会很相似。\n",
    "\n",
    "- 二次采样：对于出现频率高的词，比如介词the，所有的名词都可能跟the 邻近，但这些名词词向量肯定大同小异，因此去掉高频词，训练出的词向量效果更好。\n",
    "\n",
    "- 负采样：求解中心词出现的条件下背景词出现的概率，需要两个词向量的内积，除以中心词和其他所有词的内积之和，并做Softmax激活。这样feed propagation，back propagation都要计算 $O$(词典大小) 时间复杂度。引入负样本的思想，以窗口为界，中心词和窗口内词为正样本，中心词和窗口外词为负样本，只要计算和求导只要更新少量词的向量即可。\n",
    "\n",
    "  负采样改变了目标函数，正、负样本分别做词向量内积，用Sigmoid激活之后求积。有点类似Logistic Regression里面交叉熵损失函数，去掉指数后的乘积。\n",
    "\n",
    "    1. trick：按窗口大小随机取s = [1, window_size]，正样本一次取s个，让模型更关注距离中心词更近的词。\n",
    "    2. 负样本选取：词出现频数的0.75 次幂作为权重，一个正样本，对词典随机选取K个负样本。减弱频次差异的影响，让小概率词被采样概率变大。\n",
    "    \n",
    "\n",
    "#### 推导\n",
    "\n",
    "每个词被表示成两个$d$维向量，词在词典中索引为$i$，当它为中心词时向量表示为$\\boldsymbol{v}_i\\in\\mathbb{R}^d$，而为背景词时向量表示为$\\boldsymbol{u}_i\\in\\mathbb{R}^d$。词典索引集$\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$。\n",
    "\n",
    "设中心词$w_c$在词典中索引为$c$，背景词$w_o$在词典中索引为$o$，给定中心词生成背景词的条件概率：\n",
    "$$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)} \\tag{1}$$\n",
    "\n",
    "给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为$m$时，最大似然函数即给定任一中心词生成所有背景词的概率：\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}) \\tag{2}$$\n",
    "\n",
    "等价于最小化损失函数：\n",
    "$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, P(w^{(t+j)} \\mid w^{(t)})$$\n",
    "\n",
    "随机梯度下降，每次迭代随机采样较短子序列，计算该子序列的损失，然后计算梯度更新模型参数。梯度计算的关键是，条件概率的对数，对中心词向量和背景词向量的梯度。\n",
    "$$\\log P(w_o \\mid w_c) =\n",
    "\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)\\right)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\text{log}\\, P(w_o \\mid w_c)}{\\partial \\boldsymbol{v}_c} \n",
    "&= \\boldsymbol{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_j^\\top \\boldsymbol{v}_c)\\boldsymbol{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\\\\n",
    "&= \\boldsymbol{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\boldsymbol{u}_j^\\top \\boldsymbol{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\right) \\boldsymbol{u}_j\\\\ \n",
    "&= \\boldsymbol{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\boldsymbol{u}_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "训练结束后，对于词典中的任一索引为$i$的词，我们均得到该词作为中心词和背景词的两组词向量$\\boldsymbol{v}_i$和$\\boldsymbol{u}_i$。在自然语言处理应用中，一般使用中心词向量作为词的表征向量。\n",
    "\n",
    "对中心词$w_c$，计算$\\boldsymbol{v}_c$的梯度，需要词典中所有以$w_c$为中心词的条件概率，每一步梯度计算时间复杂度是$\\boldsymbol{O}(|\\mathcal{V}|)$，引入负采样解决此问题。\n",
    "\n",
    "设背景词$w_o$出现在中心词$w_c$的背景窗口为事件$P$，分布$P(w)^{0.75}$作为权重，采样$K$个未出现在该背景窗口中的噪声词。噪声词$w_k$（$k=1, \\ldots, K$）不出现在中心词$w_c$的该背景窗口为事件$N_k$。假设含有正样本和负样本的事件$P, N_1, \\ldots, N_K$相互独立，负采样最大化联合概率与式(2)相同。条件概率跟式(1)不同，近似表示为：\n",
    "$$ P(w^{(t+j)} \\mid w^{(t)}) = P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
    "\n",
    "$D = 1$ 表示正样本，$D = 0$ 表示负样本。\n",
    "\n",
    "对事件$P$的概率，用Sigmoid激活，\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c) = \\frac{1}{1 + \\exp(-\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}$$\n",
    "\n",
    "对事件$N$的概率，\n",
    "$$P(D=0\\mid w_c, w_o) = 1 - P(D=1\\mid w_c, w_o) = 1 - \\sigma(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c) = \\frac{1}{1 + \\exp(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}$$\n",
    "\n",
    "设文本序列中时间步$t$的词$w^{(t)}$在词典中的索引为$i_t$，噪声词$w_k$在词典中的索引为$h_k$。条件概率的对数损失为：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^\\top \\boldsymbol{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\boldsymbol{u}_{h_k}^\\top \\boldsymbol{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\boldsymbol{u}_{i_{t+j}}^\\top \\boldsymbol{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\boldsymbol{u}_{h_k}^\\top \\boldsymbol{v}_{i_t}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "训练中每步的梯度计算开销不再与词典大小相关，而与$K$线性相关。当$K$取较小的常数时，负采样在每一步的梯度计算开销较小。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
