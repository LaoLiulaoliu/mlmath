{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵和行列式的几何意义$^{[1]}$\n",
    "\n",
    "矩阵的初等行变换（相当于给矩阵左乘矩阵$A$）在几何空间的变化，伸缩或旋转。\n",
    "\n",
    "矩阵$A$ 的行列式的几何意义是线性变换$A$ 下的图形面积或体积的伸缩因子。\n",
    "\n",
    "1. 矩阵初等变换，用非零常数乘以矩阵某一行。\n",
    "\n",
    "    一个向量 $X$ 左乘矩阵 $A$：\n",
    "    \n",
    "    $A \\cdot X =$\n",
    "    $\\begin{bmatrix}\n",
    "      3 & 0 \\\\\n",
    "      0 & 0.5 \\\\\n",
    "    \\end{bmatrix}$\n",
    "    $\\cdot$\n",
    "    $\\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix} = \\begin{bmatrix} 3x\\\\ 0.5y\\\\ \\end{bmatrix}$\n",
    "\n",
    "    $A$ 为对角阵时，主对角线上每一个元素，对一个维度进行拉伸变换，$x$ 拉伸为3倍，$y$ 缩小为0.5倍。\n",
    "\n",
    "    经过变换，$x$ 和$y$ 张成矩形面积增加1.5倍，矩阵的行列式$|A| = 1.5$。\n",
    "\n",
    "2. 矩阵初等变换，互换矩阵的两行。\n",
    "\n",
    "    $A \\cdot X =$\n",
    "    $\\begin{bmatrix}\n",
    "      0 & 1\\\\\n",
    "      1 & 0\\\\\n",
    "      \\end{bmatrix}$\n",
    "    $\\cdot$\n",
    "    $\\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix} = \\begin{bmatrix} y\\\\ x\\\\ \\end{bmatrix}$\n",
    "\n",
    "    沿着$y$ = $x$ 做了一个对称旋转，高维中，是其中两维组成的对称轴做对称旋转。\n",
    "\n",
    "    经过变换，$x$ 和$y$ 张成矩形面积增加-1倍，矩阵的行列式$|A| = -1$。\n",
    "\n",
    "3. 矩阵初等变换，第i行k倍加到第j行。\n",
    "\n",
    "    $A \\cdot X =$\n",
    "    $\\begin{bmatrix}\n",
    "      1 & 2\\\\\n",
    "      0 & 1\\\\\n",
    "      \\end{bmatrix}$\n",
    "    $\\cdot$\n",
    "    $\\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix} = \\begin{bmatrix} x+2y\\\\ y\\\\ \\end{bmatrix}$\n",
    "\n",
    "    第二行的两倍加到第一行，$y$轴沿竖直方向大小不变，$x$轴沿水平方向上左右拉伸，$A_{12}$的为正，向右拉伸。\n",
    "\n",
    "    经过变换，$x$ 和$y$ 张成矩形面积不变，矩阵的行列式$|A| = 1$。\n",
    "\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) 上一些线性变换的展示：\n",
    "\n",
    "![](./img/eigenvalues_transformations.png)\n",
    "\n",
    "矩阵的初等行变换中，左乘矩阵的所有列向量组成一组新基，原矩阵$X$ 是在老基视角下的数值表示，变换后矩阵$Y$ 是在新基视角下的数值表示，左乘的矩阵$A$是过渡矩阵的逆$C^{-1}$，$Y =C^{-1} X$。如果老基组成的矩阵是单位矩阵$I$，$I \\cdot C = C$，那么过渡矩阵$C$ 就是新基。变换2的矩阵$A$是正交矩阵（$A \\cdot A^T = I$，任意两行点乘为零，任意行点乘自己为1，所以也叫单位正交阵），所以这个变换叫正交变换，变换前后向量长度，向量间内积不变，正交变换只是对向量用另一组标准正交基表示，向量往新基的各个方向做变换，老基是，\n",
    "$$\n",
    "(e_1, e_2) =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0\\\\\n",
    "  0 & 1\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "经过正交变换，矩阵的新基，\n",
    "$$\n",
    "(\\epsilon_1, \\epsilon_2) =\n",
    "  \\begin{bmatrix}\n",
    "  0 & 1\\\\\n",
    "  1 & 0\\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "相似矩阵是一个线性变换在不同基下的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVD\n",
    "\n",
    "1. 特征值、特征向量定义：\n",
    "\n",
    "    对n阶方阵$A$，如果存在一个数$\\lambda$和一个n维非零列向量$v$，使得$A \\cdot v = \\lambda \\cdot v$，则$\\lambda$是矩阵$A$的一个特征值，$v$是特征值$\\lambda$下的特征向量。\n",
    "\n",
    "2. 特征值、特征向量的几何意义：\n",
    "\n",
    "   对特征向量$v$做初等行变换（左乘$A$），等于对特征向量$v$伸缩$\\lambda$倍。特征值表示伸缩系数，特征向量是只有伸缩变换，没有旋转变换的线性变换中的方向向量。\n",
    "\n",
    "3. 线性代数知识\n",
    "\n",
    "    方阵$A$有$k$个特征值和特征向量，则\n",
    "    $$\n",
    "    A (v_1, v_2, ..., v_k) = (\\lambda_1 v_1, \\lambda_2 v_2, ..., \\lambda_k v_k) = (v_1, v_2, ..., v_k)\n",
    "    \\begin{bmatrix}\n",
    "    \\lambda_1 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & \\cdots & \\lambda_k \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "    A V = V \\Lambda => A = V \\Lambda V^{-1}\n",
    "    $$\n",
    "    \n",
    "    方阵$A$可被相似对角化，充要条件是$A$有$n$个线性无关的特征向量，相似对角矩阵$\\Lambda = V^{-1} A V$。$n$个线性无关的特征向量，构成相似变换矩阵$V$，$V$可逆。$V$ 对$A$ 做相似变换。\n",
    "\n",
    "    若$A$ 可相似对角化，$n$个特征值构成相似对角阵$\\Lambda$的主对角线元素。$n$个特征值不同，则对应$n$个线性无关特征向量。特征值$\\lambda_i$有$k$重根，$(A - \\lambda_i I) x = 0$所对应的基础解系解向量的个数也是$k$，即特征值$\\lambda_i$对应$k$个线性无关的特征向量（总共保证$n$个线性无关特征向量），其中$k = n - r(A - \\lambda_i I)$。\n",
    "\n",
    "特征值分解（Eigen value decomposition）就是把矩阵用相似对角阵表示，$A = Q \\Sigma Q^{-1} = V \\Lambda V^{-1}$。\n",
    "\n",
    "对角矩阵$\\Sigma$ 的主对角线元素是特征值，特征值从大到小排列，特征值所对应的线性无关特征向量，在Q中以列向量的形式从左到右排列。\n",
    "在高维空间中，矩阵$A$表示一个线性变换，$n$个特征向量表示矩阵最重要的$n$个变换方向，用这$n$个变换方向可以近似矩阵（变换），特征值表示这个特征的重要程度。\n",
    "\n",
    "$A$是实对称矩阵，则$n$个特征向量两两正交。$Q$ 对$A$ 做正交变换，同时也是相似变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "1. 奇异值分解（Singular value decomposition）比特征值分解更通用，不要求所变换矩阵是方阵。\n",
    "    $$\n",
    "    A_{m x n} = U_{m x m} \\cdot \\Sigma_{m x n} \\cdot V^{T}_{n x n}\n",
    "    $$\n",
    "    \n",
    "    $U$ 是左奇异矩阵，正交阵，$u_i$ 是$U$ 的列向量，称左奇异向量；\n",
    "    \n",
    "    $V$ 是右奇异矩阵，正交阵，$v_i$ 是$V$ 的列向量，称右奇异向量；\n",
    "    \n",
    "    $\\Sigma$ 仅在主对角线上有值，称为奇异值，其他元素为零。\n",
    "\n",
    "2. 线性代数\n",
    "\n",
    "    * $(A^T A)^T = A^T A$，$A^T A$ 在实数域是$n x n$ 实对称矩阵。\n",
    "    \n",
    "    * 实对称矩阵，必存在$n$个线性无关且相互正交的特征向量。\n",
    "    \n",
    "    * 正交矩阵，根据定义，逆等于转置。\n",
    "    \n",
    "    * $r(A^T A) = r(A) = r(\\Sigma) = k, (k \\leq min\\{m, n\\})$。即$A^T A$ 非零特征值个数为$k$，重根重复计数。（初等变换不改变矩阵的秩）\n",
    "    \n",
    "    * 矩阵的秩和特征值个数没有关系。\n",
    "    \n",
    "    * 根据EVD的推导，$A$是对称方阵且可相似对角化时，$U = V$，特征值等于奇异值。\n",
    "    \n",
    "    * 奇异值等于特征值的算术平方根，所以是非负实数。\n",
    "    \n",
    "    * 矩阵$A$ 的零空间是特征值为零所对应的特征向量空间，此时$A$为奇异矩阵，条件数为无穷，$A$ 把零空间的特征向量线性变换到零点。\n",
    "    \n",
    "3. 奇异值分解几何意义：\n",
    "\n",
    "    对于任何矩阵$A$，我们都能找到一组标准正交基，它是由原来标准正交基经过伸缩和旋转得到。老基经过$A$变换得到新基，$A$将$n$维空间中向量，映射到$k$维空间中。\n",
    "\n",
    "4.  奇异值分解推导$^{[2]}$\n",
    "\n",
    "    $A^T A$ 的$n$个线性无关且相互正交的特征向量，$V = (v_1, v_2, \\dots, v_n)$，$v_i$为$n$维列向量，\n",
    "\n",
    "    $$\n",
    "    (A^T A) v_i = \\lambda_i v_i =\n",
    "    \\begin{cases}\n",
    "    \\lambda_i v_i, &i \\leq k \\\\\n",
    "    0, &i > k\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    当$i, j \\leq k$时，等式两边左乘$v_j^T$，\n",
    "    \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    v_j^T \\cdot A^T A v_i &= v_j^T \\lambda_i v_i \\\\\n",
    "    (A v_j)^T \\cdot A v_i &= \\lambda_i v_j^T v_i =\n",
    "    \\begin{cases}\n",
    "    0, & i \\neq j \\\\\n",
    "    \\lambda_i, & i = j\n",
    "    \\end{cases}\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "    即 $|A v_i| = \\sqrt{\\lambda_i}, \\quad A v_i \\bot A v_j$\n",
    "    \n",
    "    $A v_1, A v_2, ..., A v_k$是一组相互正交的$m$维列向量，将这组正交基正交规范化，\n",
    "    \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{A v_1}{|A v_1|}, \\frac{A v_2}{|A v_2|}, ..., \\frac{A v_k}{|A v_k|} &= \\frac{A v_1}{\\sqrt{\\lambda_1}}, \\frac{A v_2}{\\sqrt{\\lambda_2}}, ..., \\frac{A v_k}{\\sqrt{\\lambda_k}} \\\\\n",
    "    &= u_1, u_2, ..., u_k\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "    因为$k \\leq m$ 且基是$m$维的，用扩展基定理，把这组规范正交基扩展到$m$个$m$ 维正交基，$u_1, u_2, ..., u_k, u_{k+1}, ..., u_m$。\n",
    "    \n",
    "    $u_{k+1}, ..., u_m$ 因为对应分母$\\lambda_{k+1}, ..., \\lambda_m$ 不能为零，所以不是$\\frac{A v_k}{\\sqrt{\\lambda_k}}$ 的形式，它们是矩阵零空间$^{[3]}$中的特征向量。因而，\n",
    "    $$\n",
    "    u_i \\sqrt{\\lambda_i}= A v_i \\quad (i, j \\leq k)\\\\\n",
    "    u_i \\cdot u_j =\n",
    "    \\begin{cases}\n",
    "    0, & i \\neq j \\\\\n",
    "    1, & i = j\n",
    "    \\end{cases}\n",
    "    \\quad (i, j \\leq m)\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    A (v_1, v_2, ... v_n) = (\\frac{A v_1}{\\sqrt{\\lambda_1}}, \\frac{A v_2}{\\sqrt{\\lambda_2}}, ..., \\frac{A v_k}{\\sqrt{\\lambda_k}}, ..., u_{k+1}, ..., u_m)\n",
    "    \\begin{bmatrix}\n",
    "    \\sqrt{\\lambda_1} & & & & & & \\\\\n",
    "     & \\sqrt{\\lambda_2} & & & & & \\\\\n",
    "     & & \\ddots & & & & \\\\\n",
    "     & & & \\sqrt{\\lambda_k} & & & \\\\\n",
    "     & & & & 0 & & \\\\\n",
    "     & & & & & \\ddots & \\\\\n",
    "     & & & & & & 0\n",
    "    \\end{bmatrix}\n",
    "    = U \\Sigma_{m x n} \\\\\n",
    "    A = U \\Sigma V^{-1} = U \\Sigma V^T\n",
    "    $$\n",
    "    \n",
    "    奇异值$\\sigma_i = \\sqrt{\\lambda_i} = \\frac{A v_i}{u_i}$。\n",
    "    \n",
    "    推导矩阵$U$ 的列向量$u_i$ 是$A A^T$ 的$m$ 个正交特征向量。\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\because  u_i \\sqrt{\\lambda_i} &= A v_i \\\\\n",
    "    \\therefore A A^T u_i &= A A^T \\frac{A v_i}{\\sqrt{\\lambda_i}} = A \\frac{A^T A v_i}{\\sqrt{\\lambda_i}} = A \\frac{\\lambda_i v_i}{\\sqrt{\\lambda_i}} = \\lambda_i \\frac{A v_i}{\\sqrt{\\lambda_i}} = \\lambda_i u_i \\\\\n",
    "    A A^T u_i &= \\lambda_i u_i\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "5. 根据奇异值分解公式，反推奇异值和特征值关系\n",
    "    $$\n",
    "    (A A^T) = (U \\Sigma V^T) \\cdot (V \\Sigma^T U^T) = U \\Sigma \\Sigma^T U^T \\\\\n",
    "    (A^T A) = (V \\Sigma^T U^T) \\cdot (U \\Sigma V^T) = V \\Sigma^T \\Sigma V^T\n",
    "    $$\n",
    "\n",
    "    $\\Sigma \\Sigma^T$ 是$m x m$，$\\Sigma^T \\Sigma$ 是$n x n$，但他们主对角线上的值都相等，等于$\\Sigma$ 主对角线奇异值的平方，所以$\\Sigma \\Sigma^T = \\Sigma^2_{m x m}$，$\\Sigma^T \\Sigma = \\Sigma^2_{n x n}$。若$m < n$，则$\\Sigma$ 主对角线上元素个数$<= m$。\n",
    "\n",
    "    $$\n",
    "    (A A^T) = U \\Sigma^2 U^T = U \\Sigma^2 U^{-1} \\\\\n",
    "    (A^T A) = V \\Sigma^2 V^T = V \\Sigma^2 V^{-1}\n",
    "    $$\n",
    "\n",
    "    根据相似对角化公式，$(A A^T)$ 和$(A^T A)$ 的特征值是矩阵$A$ 奇异值的平方，$\\lambda_i = \\sigma^2_i$，奇异值是特征值的平方根，$\\sigma_i = \\sqrt{\\lambda_i}$。\n",
    "\n",
    "6. 奇异值分解的过程\n",
    "\n",
    "    (1). 用特征值分解求$V$；(2). 根据$V$ 的特征值，求奇异值；(3). 根据$u_i = \\frac{A v_i}{\\sqrt{\\lambda_i}}$ 求$U$。\n",
    "    \n",
    "    因为特征向量不唯一，特征向量来自齐次线性方程组的解。所以根据$(A A^T) u_i = \\lambda_i u_i$ 求出的$U$，未必能满足奇异值分解公式。如果等式两边$u_i$ 为负，等式依旧成立。\n",
    "    \n",
    "    $A = U \\Sigma V^T => U = A (\\Sigma V^T)^{-1} = A V \\Sigma^{-1}$\n",
    "    \n",
    "7. SVD 应用\n",
    "\n",
    "    奇异值在矩阵$\\Sigma$ 主对角线上从大到小排列，奇异值的平方--特征值在矩阵$\\Sigma^2$ 上从大到小排列，特征值所对应的特征向量，分别在$U$，$V$中以列向量的形式从左到右排列。\n",
    "\n",
    "    奇异值分解中，大部分情况下，前一小部分奇异值之和，占奇异值总和99%以上，故常用前k个奇异值近似描述矩阵。\n",
    "\n",
    "    $$\n",
    "    A_{m x n} = U_{m x m} \\cdot \\Sigma_{m x n} \\cdot V^{T}_{n x n} \\approx U_{m x k} \\cdot \\Sigma_{k x k} \\cdot V^T_{k x n} \\quad  (k << n)\n",
    "    $$\n",
    "\n",
    "    奇异值分解成三个小矩阵相乘，近似原矩阵，达到降维目的，减少了空间存储，可做有损压缩。奇异值和特征值表示数据信息量，数值越大，数据信息量越大。\n",
    "\n",
    "    SVD计算举例，可以看这篇文章 [机器学习中SVD总结](https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg) -- 2.2节。\n",
    "    \n",
    "    SVD做图片有损压缩代码：\n",
    "\n",
    "```python\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = sklearn.datasets.load_sample_images().images\n",
    "shape0, shape1, shape2 = images[0].shape\n",
    "print(len(images), (shape0, shape1, shape2)) # 2 (427, 640, 3)\n",
    "\n",
    "data = images[0].reshape(shape0, shape1 * shape2) / 255. # (427, 1920)\n",
    "U, Sigma, VT = np.linalg.svd(data) # (427, 427) (427,) (1920, 1920)\n",
    "\n",
    "dim = 100\n",
    "reconstruction = np.matmul(U[:, :dim], np.diag(Sigma[:dim])).dot(VT[:dim, :]) # (427, 1920)\n",
    "reconstruction = reconstruction.reshape(shape0, shape1, shape2)\n",
    "\n",
    "print(dim / shape0, Sigma[:dim].sum() / Sigma.sum()) # 0.234192037470726 0.7180043485723469\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax[0].imshow(images[0])\n",
    "ax[1].imshow(reconstruction)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![svd_lossy_compression](./img/svd_lossy_compression.png)\n",
    "\n",
    "    随着奇异值占比的增加，图片越来越清晰，跟原图片每个像素值的均方误差之和越小。\n",
    "    \n",
    "8. 伪逆\n",
    "\n",
    "    $n$ 阶方阵$A$ 可逆的充要条件是，秩为$n$，行列式不为零，是非奇异矩阵。伪逆（pseudoinverse）将求逆泛化到行列不等的矩阵中，也称Generalized Inverse，Moore-Penrose Inverse。\n",
    "    \n",
    "    伪逆定义来源于奇异值分解，$A^+ = V_{n x n} \\cdot D^+_{n x m} \\cdot U^T_{m x m}$，\n",
    "    \n",
    "    其中$D^+_{n x m}$ 是 $\\Sigma_{m x n}$ 主对角线非零元素求倒数，再转置。可以通过奇异值分解，来求伪逆。伪逆有更全面的定义，Google之。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] [【线性代数的几何意义】行列式的几何意义](https://www.cnblogs.com/AndyJee/p/3491487.html)\n",
    "\n",
    "[2] [SVD推导](https://blog.csdn.net/qq_25847123/article/details/79358598)\n",
    "\n",
    "[3] [矩阵的奇异值(SVD)分解及其简单应用](https://zhuanlan.zhihu.com/p/183065884)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
