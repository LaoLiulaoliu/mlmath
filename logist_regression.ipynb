{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logist Regression\n",
    "\n",
    "#### Deduction\n",
    "Input: $$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 & \\cdots & 3\\\\\n",
    "  4 & 4 & \\cdots & 6\\\\\n",
    "  7 & 8 & \\cdots & 9\\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  11 & 12& \\cdots & 13\\\\\n",
    "  \\end{bmatrix}_{m*n}\n",
    ",\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "  1\\\\\n",
    "  0\\\\\n",
    "  1\\\\\n",
    "  \\vdots\\\\\n",
    "  1\\\\\n",
    "  \\end{bmatrix}_{m*1}\n",
    "$$\n",
    "\n",
    "Hypothesis: $$\n",
    "  H_\\theta(x) = g(z) = \\frac{1}{1+e^{-z}} = \\frac{e^z}{1+e^z}\n",
    "  = \\frac{1}{1+e^{-\\theta^T X}} \\tag{1}\n",
    "$$\n",
    "\n",
    "  \n",
    "Linear function: $$\n",
    "  z = \\theta^T X, \\quad z \\in (-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Logist/Sigmoid function: $$\n",
    "  g(z) = \\frac{1}{1+e^{-z}} = \\frac{e^z}{1+e^z}, \\quad g(z) \\in (0, 1)\n",
    "$$\n",
    "\n",
    "| z $\\geq$ 0 | z < 0 |\n",
    "| :---: | :---: |\n",
    "| $\\theta^T X$ $\\geq$ 0 | $\\theta^T X$ < 0 |\n",
    "| $h_\\theta(x)$ $\\geq$ 0.5 | $h_\\theta(x)$ < 0.5 |\n",
    "| **g(z)** $\\geq$ 0.5 | **g(z)** < 0.5 |\n",
    "| **y** = 1 | **y** = 0 |\n",
    "\n",
    "![Sigmoid](./img/sigmoid.jpg)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-10, 10, 1000)\n",
    "g = 1 / (1 + np.e ** -z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "sns.lineplot(x='z', y='g(z)', ax=ax, data={'z': z, 'g(z)': g})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$H_\\theta(x)$ 被Sigmoid函数归一化到(0,1)，表示结果分类结果为True的概率。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y=1|x;\\theta) &= H_\\theta(x) \\\\\n",
    "P(y=0|x;\\theta) &= 1 - H_\\theta(x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Assume $y|x;\\theta \\sim Bernoulli(1, p)$，合并出y的概率质量函数（y在各特定取值上的概率总和为1）:<br> $$\n",
    "\\begin{align}\n",
    "  P(y|x;\\theta) &= p^y (1-p)^{1-y} \\\\\n",
    "                &= \\begin{cases} p, \\quad &y = 1 \\\\\n",
    "                                 1 - p, \\quad &y = 0\n",
    "                   \\end{cases} \\\\\n",
    "                &= (H_\\theta(x))^y (1 - H_\\theta(x))^{1-y} \\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Maximum likelihood function是数据集出现概率最大的函数，一个事件已经观察到的维度发生的概率越大，那么未观察到的维度（误差）发生的概率越小。\n",
    "\n",
    "所有样本的贡献相互独立, 数据集的概率是每一个样本概率之乘积: $$\n",
    "  L(\\theta) = \\prod_{j=0}^{m} p(y^j|x^j;\\theta)\n",
    "$$\n",
    "\n",
    "获得最大化似然的参数, 是对$\\theta$的参数估计: $$\n",
    "\\begin{align}\n",
    "  \\theta^* = \\operatorname*{arg\\ max}_{\\theta}\\ L(\\theta) &= \\prod_{j=0}^{m} p(y^j|x^j;\\theta) \\\\\n",
    "  &= \\prod_{j=0}^{m} (H_\\theta(x)^j)^y \\, (1-H_\\theta(x)^j)^{1-y} \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Log transformation，1.连乘变成连加方便求导，2.概率很小时候连乘容易等于0: $$\n",
    "\\begin{align}\n",
    "  \\ell(\\theta) &= \\log(L(\\theta)) \\\\\n",
    "  &= \\sum_{j=0}^{m} y^j\\ log(H_\\theta(x)^j) + (1-y^j)\\ log(1-H_\\theta(x)^j) \\tag{4} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Binary cross engropy(minimize, 预测分布越接近真实分布): $$\n",
    "  -\\ell(\\theta) = -\\sum_{j=0}^{m} y^j\\ log(H_\\theta(x)^j) + (1-y^j)\\ log(1-H_\\theta(x)^j)\n",
    "$$\n",
    "\n",
    "Partial Derivative for every $\\theta$: $$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} &= \\sum_{j=0}^{m} (y^j \\frac{1}{H_\\theta(x)^j} + (1-y^j) \\frac{0-1}{1-H_\\theta(x)^j}) \\frac{\\partial}{\\partial \\theta_i} H_\\theta(x)^j \\\\\n",
    "        &= \\sum_{j=0}^{m} (y^j \\frac{1}{H_\\theta(x)^j} - (1-y^j) \\frac{1}{1-H_\\theta(x)^j}) \\frac{\\partial}{\\partial \\theta_i} H_\\theta(x)^j \\\\\n",
    "  \\because g'(z) &= \\frac{e^{-z}}{(1+e^{-z})^2} \\\\\n",
    "        &= \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} \\\\\n",
    "        &= g(z) \\cdot (1-g(z)) \\\\\n",
    "  \\therefore \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} &= \\sum_{j=0}^{m} (y^j \\frac{1}{H_\\theta(x)^j} - (1-y^j) \\frac{1}{1-H_\\theta(x)^j})\\ H_\\theta(x)^j\\ (1-H_\\theta(x)^j)\\ x_i^j \\\\\n",
    "  &= \\sum_{j=0}^{m} (y^j(1-H_\\theta(x)^j) - (1-y^j)H_\\theta(x)^j)\\ x_i^j \\\\\n",
    "  &= \\sum_{j=0}^{m} (y^j - H_\\theta(x)^j)\\ x_i^j \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gradient ascent to optimise $\\theta$: $$\n",
    "\\begin{align}\n",
    "  \\theta_i &= \\theta_i + \\frac{\\partial log(L(\\theta))}{\\partial \\theta_i} \\\\\n",
    "           &= \\theta_i + \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} \\\\\n",
    "           &= \\theta_i + \\sum\\limits_{j=0}^m (y^j - H_\\theta(x)^j) x_i^j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gradient decend: $$\n",
    "\\theta_i = \\theta_i - \\sum\\limits_{j=0}^m (H_\\theta(x)^j - y^j) x_i^j \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "1. 设定阈值。Logist regression 计算出的是一个类似概率的值，不一定大于0.5就判断为正，可以根据实际情况设定。\n",
    "2. 置信度，除了预测出分类，还能给出分类结果的置信度。\n",
    "3. 对数据倾斜敏感，正负数据比相差悬殊时预测效果不好。损失函数二值交叉熵越小，代表预测分布越接近真实分布，预测越准确。最大似然估计，是所有样本被预测正确的最大概率。样本集正例只有几个，几乎都是负例，则很容易预测准确率很高，但正例预测对的概率很低，此时要对数据重采样/欠采样。\n",
    "\n",
    "#### Logist Function\n",
    "因变量和自变量之间的多元线性关系：$Y = \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_n x_n, \\quad \\sum\\limits_{i=0}^n \\theta_i x_i \\in (-\\infty, +\\infty) $\n",
    "\n",
    "想利用线性回归模型演化出逻辑回归，要把$(-\\infty, +\\infty)$的范围归一化到(0,1)上，对数函数是把(0,1)的自变量映射到$(-\\infty, 0)$上，$log(Y) = \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_n x_n$，这时如果$Y \\in (0,1)，log(Y) \\in (-\\infty, 0)$，还少一半对称的范围。\n",
    "\n",
    "![Log](./img/log.png)\n",
    "```python\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.log(x)\n",
    "p = figure(title=\"log(x)\", x_axis_label='x', x_range=(-10,10), y_range=(-2,2), plot_height=300)\n",
    "p.line(x, y, line_width=2)\n",
    "show(p)\n",
    "```\n",
    "\n",
    "几率（odd）是事件发生与不发生概率的比，假设事件E发生的概率为p：$$\n",
    "odd(E) = \\frac{p}{1-p} = \\frac{1}{\\frac{1}{p}-1} \\\\\n",
    "  \\because p \\in (0,1) \\\\\n",
    "  \\therefore \\frac{p}{1-p} \\in (0, +\\infty) => log(\\frac{p}{1-p}) \\in (-\\infty, +\\infty)\n",
    "$$\n",
    "\n",
    "![Odd](./img/odd.png)\n",
    "![Logit](./img/logit.png)\n",
    "\n",
    "得出logit变换：$$log(\\frac{p}{1-p}) = \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_n x_n = \\sum\\limits_{i=0}^n \\theta_i x_i \\tag{7}\n",
    "$$\n",
    "log是指数变换，it是odd函数，logit变换就是对odd函数做指数变换。\n",
    "\n",
    "对logit变换函数求反函数，得出logist function：$$\n",
    "\\frac{p}{1-p} = e^{\\theta^T X} \\\\\n",
    "p = \\frac{e^{\\theta^T X}}{1+e^{\\theta^T X}}\n",
    "$$\n",
    "\n",
    "#### Maximum likelihood function $^{[1]}$\n",
    "概率是某事件发生之前，根据环境参数对其发生可能性的预测。比如抛硬币之前，根据硬币是密度均匀分布推测正反面概率都是50%。\n",
    "\n",
    "似然是某事件发生后，根据确定的结果推测产生结果的环境参数。比如抛1000次硬币，496次朝上，504次朝下，可以大致推断出硬币密度分布均匀，抛硬币的人没有作弊。\n",
    "\n",
    "如果用$\\theta$表示环境参数，x表示结果，概率可表示为，$P(x|\\theta)$，似然可以表示为，$L(\\theta|x)$.\n",
    "\n",
    "最大似然估计函数是样本的函数，根据样本的观测值$x_1,x_2,...x_n,$带入统计表达式，得到最大似然估计值。\n",
    "\n",
    "\n",
    "### References\n",
    "[1] [最大似然估计、n阶矩、协方差（矩阵）、（多元）高斯分布 学习摘要](https://www.cnblogs.com/boceng/p/11358480.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
