{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logist Regression\n",
    "\n",
    "Input: $$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 & \\cdots & 3\\\\\n",
    "  4 & 4 & \\cdots & 6\\\\\n",
    "  7 & 8 & \\cdots & 9\\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  11 & 12& \\cdots & 13\\\\\n",
    "  \\end{bmatrix}_{m*n}\n",
    ",\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "  1\\\\\n",
    "  0\\\\\n",
    "  1\\\\\n",
    "  \\vdots\\\\\n",
    "  1\\\\\n",
    "  \\end{bmatrix}_{m*1}\n",
    "$$\n",
    "\n",
    "Hypothesis: $$\n",
    "  H_\\theta(x) = g(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-\\theta^T X}} \\tag{1}\n",
    "$$\n",
    "\n",
    "  \n",
    "Linear function: $$\n",
    "  z = \\theta^T X, \\quad z \\in (-\\infty, \\infty)\n",
    "$$\n",
    "\n",
    "Sigmoid function: $$\n",
    "  g(z) = \\frac{1}{1+e^{-z}}, \\quad g(z) \\in (0, 1)\n",
    "$$\n",
    "\n",
    "| z $\\geq$ 0 | z < 0 |\n",
    "| :---: | :---: |\n",
    "| $\\theta^T X$ $\\geq$ 0 | $\\theta^T X$ < 0 |\n",
    "| $h_\\theta(x)$ $\\geq$ 0.5 | $h_\\theta(x)$ < 0.5 |\n",
    "| **g(z)** $\\geq$ 0.5 | **g(z)** < 0.5 |\n",
    "| **y** = 1 | **y** = 0 |\n",
    "\n",
    "![Sigmoid](./sigmoid.jpg)\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-10, 10, 1000)\n",
    "g = 1 / (1 + np.e ** -z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "sns.lineplot(x='z', y='g(z)', ax=ax, data={'z': z, 'g(z)': g})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$H_\\theta(x)$ 被Sigmoid函数归一化到(0,1)，表示结果分类结果为True的概率。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y=1|x;\\theta) &= H_\\theta(x) \\\\\n",
    "P(y=0|x;\\theta) &= 1 - H_\\theta(x)\n",
    "\\end{align}\n",
    "$$\n",
    "Combine: $$\n",
    "P(y|x;\\theta) = (H_\\theta(x))^y (1 - H_\\theta(x))^{1-y} \\tag{2}\n",
    "$$\n",
    "\n",
    "\n",
    "Maximum likelihood function是数据集出现概率最大的函数, 所有样本相互独立, 数据集的概率是每一个样本概率之乘积: $$\n",
    "  L(\\theta) = \\prod_{j=0}^{m} p(y^{(j)}|x^{(j)};\\theta)\n",
    "$$\n",
    "\n",
    "获得最大化似然的参数, 是对$\\theta$的参数估计: $$\n",
    "\\begin{align}\n",
    "  \\theta^* = \\operatorname*{arg\\ max}_{\\theta}\\ L(\\theta) &= \\prod_{j=0}^{m} p(y^{(j)}|x^{(j)};\\theta) \\\\\n",
    "  &= \\prod_{j=0}^{m} (H_\\theta(x)^{(j)})^y \\, (1-H_\\theta(x)^{(j)})^{1-y} \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Log transformation, the form of cross engropy: $$\n",
    "\\begin{align}\n",
    "  \\ell(\\theta) &= \\log(L(\\theta)) \\\\\n",
    "  &= \\sum_{j=0}^{m} y^{(j)}\\ log(H_\\theta(x)^{(j)}) + (1-y^{(j)})\\ log(1-H_\\theta(x)^{(j)}) \\tag{4} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Partial Derivative for every $\\theta$: $$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} &= \\sum_{j=0}^{m} (y^{(j)} \\frac{1}{H_\\theta(x)^{(j)}} + (1-y^{(j)}) \\frac{0-1}{1-H_\\theta(x)^{(j)}}) \\frac{\\partial}{\\partial \\theta_i} H_\\theta(x)^{(j)} \\\\\n",
    "        &= \\sum_{j=0}^{m} (y^{(j)} \\frac{1}{H_\\theta(x)^{(j)}} - (1-y^{(j)}) \\frac{1}{1-H_\\theta(x)^{(j)}}) \\frac{\\partial}{\\partial \\theta_i} H_\\theta(x)^{(j)} \\\\\n",
    "  \\because g'(z) &= \\frac{e^{-z}}{(1+e^{-z})^2} \\\\\n",
    "        &= \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} \\\\\n",
    "        &= g(z) \\cdot (1-g(z)) \\\\\n",
    "  \\therefore \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} &= \\sum_{j=0}^{m} (y^{(j)} \\frac{1}{H_\\theta(x)^{(j)}} - (1-y^{(j)}) \\frac{1}{1-H_\\theta(x)^{(j)}})\\ H_\\theta(x)^{(j)}\\ (1-H_\\theta(x)^{(j)})\\ x_i^{(j)} \\\\\n",
    "  &= \\sum_{j=0}^{m} (y^{(j)}(1-H_\\theta(x)^{(j)}) - (1-y^{(j)})H_\\theta(x)^{(j)})\\ x_i^{(j)} \\\\\n",
    "  &= \\sum_{j=0}^{m} (y^{(j)} - H_\\theta(x)^{(j)})\\ x_i^{(j)} \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gradient ascent to optimise $\\theta$: $$\n",
    "\\begin{align}\n",
    "  \\theta_i &= \\theta_i + \\frac{\\partial log(L(\\theta))}{\\partial \\theta_i} \\\\\n",
    "           &= \\theta_i + \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} \\\\\n",
    "           &= \\theta_i + \\sum\\limits_{j=0}^m (y^{(j)} - H_\\theta(x)^{(j)}) x_i^{(j)} \\tag{6}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
