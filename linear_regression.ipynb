{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduction\n",
    "\n",
    "Input:\n",
    "$$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 & \\cdots & 3\\\\\n",
    "  4 & 4 & \\cdots & 6\\\\\n",
    "  7 & 8 & \\cdots & 9\\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  11 & 12& \\cdots & 13\\\\\n",
    "  \\end{bmatrix}_{m*n}\n",
    ",\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "  9\\\\\n",
    "  11\\\\\n",
    "  13\\\\\n",
    "  \\vdots\\\\\n",
    "  19\\\\\n",
    "  \\end{bmatrix}_{m*1}\n",
    "$$\n",
    "\n",
    "Assume $y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "Hypothesis:\n",
    "$$\n",
    "  H_\\theta(x) = \\sum\\limits_{i=0}^n \\theta_i x_i = \\theta_0 x_0 + \\theta_1 x_1 + \\ldots + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "Loss Function of all samples:\n",
    "$$\n",
    "\\ell(\\theta) = \\frac{1}{2m}\\sum_{j=0}^m (H_{\\theta}^{(j)}(x) - y^{(j)})^2\n",
    "$$\n",
    "\n",
    "minimize loss function:\n",
    "$$\n",
    "\\operatorname*{arg\\ min}_{\\theta} \\ell(\\theta)\n",
    "$$\n",
    "\n",
    "Partial Derivative for every $\\theta$:\n",
    "$$\n",
    "  \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta^{(j)}(x) - y^{(j)}) x_i^{(j)}\n",
    "$$\n",
    "\n",
    "Gradient descent optimization of $\\theta$:\n",
    "$$\n",
    "  \\theta_i = \\theta_i - \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta^{(j)}(x) - y^{(j)}) x_i^{(j)}\n",
    "$$\n",
    "\n",
    "\n",
    "在实际线性回归拟合中，对于Input，m代表数据量，n代表要拟合出的方程系数，一般$m \\gg n$。下标表示n个自变量的序号，上标表示m个样本的序号。\n",
    "\n",
    "当我们把线性回归拟合看作解方程组时，就不严谨的认为是解n个未知数$\\theta$的，m行 m个方程组。行列式\n",
    "$\\begin{vmatrix} X \\end{vmatrix}_{n*n} \\neq 0$，根据克莱姆法则，方程组有唯一解。\n",
    "\n",
    "从矩阵的角度看，非齐次线性方程组 $X\\theta = Y$，系数矩阵$X$的秩等于增广矩阵$\\overline{X}$（即$\\begin{pmatrix}XY\\end{pmatrix}$），$r(X)=r(\\overline{X})=n$，方程组有唯一解，$\\begin{vmatrix}\n",
    "X \\end{vmatrix}_{n*n}\n",
    "$是非奇异方阵。如果$r(X)=r(\\overline{X})=r<n$，方程组有无穷多组解，就是我们说的可能优化到局部最优点。\n",
    "\n",
    "从n维空间中的向量角度看，$X_1,X_2,...,X_n$\n",
    "线性无关，且是极大线性无关组，是一组基，则向量组$X$的秩（也是列秩）$r(X)=n$。$r(X)<n$，则方程有很多局部最优点。极大线性无关组不唯一，有几个极大线性无关组，就有几个全局最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降$^{[1]}$\n",
    "\n",
    "想用线性多项式函数$f(x) = a_0 + a_1 x + a_2 x^2 + ... + a_n x^n$ 拟合任一曲线函数$g(x)$，则需要满足$f$在点$a$的各阶导数与$g$在点$a$的各阶导数相等，$f$在$a$的泰勒展开式\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + \\frac{f^{(1)}(a)}{1!}(x-a)^1 + \\frac{f^{(2)}(a)}{2!}(x-a)^2 + \\ldots + \\frac{f^{(n)}(a)}{n!} (x-a)^n + \\ldots,\n",
    "$$\n",
    "\n",
    "将上式中 $x$ 和 $a$ 分别替换成$x+\\epsilon$和$x$ ，可以得到\n",
    "$$\n",
    "f(x + \\epsilon) \\approx f(x) + f'(x) \\epsilon + \\mathcal{O}(\\epsilon^2).\n",
    "$$\n",
    "\n",
    "如果$\\epsilon$足够小，上式也可以简化成\n",
    "$$\n",
    "f(x + \\epsilon) \\approx f(x) + f'(x) \\epsilon.\n",
    "$$\n",
    "\n",
    "找到一个常数$\\eta > 0$，使得$\\left|\\eta f'(x)\\right|$足够小，那么可以将$\\epsilon$替换为$-\\eta f'(x)$并得到\n",
    "$$\n",
    "f(x - \\eta f'(x)) \\approx f(x) -  \\eta f'(x)^2.\n",
    "$$\n",
    "\n",
    "如果导数$f'(x) \\neq 0$，那么$\\eta f'(x)^2>0$，所以\n",
    "$$\n",
    "f(x - \\eta f'(x)) \\lesssim f(x).\n",
    "$$\n",
    "\n",
    "这意味着，如果通过\n",
    "$$\n",
    "x \\leftarrow x - \\eta f'(x)\n",
    "$$\n",
    "来迭代$x$，函数$f(x)$的值可能会降低。因此在梯度下降中，我们先选取一个初始值$x$和常数$\\eta > 0$（称作学习率），然后不断通过上式来迭代$x$，直到达到停止条件，例如$f'(x)^2$的值已足够小或迭代次数已达到某个值。\n",
    "\n",
    "这里要求$\\eta$足够小，并且$f'(x)$ 足够小，不然假设不成立。也就是说最好对数据做归一化，不然输入值太大，导数的值也会很大。\n",
    "\n",
    "#### 什么是梯度\n",
    "\n",
    "偏导数是函数沿坐标轴方向的变化率，方向导数是函数沿着某个方向的变化率，例如函数 $f(x,y)$ 在点 $P_0(x_0,y_0)$ 可微分，那么函数在该点沿任一方向d 的方向导数存在，$e_d = (a,b)$是方向为d 的单位向量。\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial d}|(x_0,y_0) = f_x(x_0,y_0)a + f_y(x_0,y_0)b\n",
    "$$\n",
    "\n",
    "梯度是一个矢量，有大小和方向，可以理解成空间中的射线。梯度表示函数在一点处，方向导数沿着这个方向取得了最大值，方向是梯度方向，大小是函数变化率，是梯度的模。函数 $f(x,y)$ 在点 $P_0(x_0,y_0)$ 的梯度，一般写成 $grad f(x_0,y_0)$或 $\\nabla f(x_0,y_0) $。一般的梯度写为 $\\nabla f = (f_x,f_y)$。 \n",
    "\n",
    "那么方向导数和梯度关系的数学表示：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial d}|(x_0,y_0) &= f_x(x_0,y_0)a + f_y(x_0,y_0)b \\\\\n",
    "&= \\nabla f(x_0,y_0) \\cdot e_d \\\\\n",
    "&= |\\nabla f(x_0,y_0)| \\cdot cos\\beta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中 $\\beta  $ 是梯度与方向d 单位向量的夹角，当夹角为0 时，方向导数与梯度同方向，取得最大值。\n",
    "\n",
    "#### 多维梯度下降\n",
    "\n",
    "目标函数的输入为向量，输出为标量。在多维空间中，比如n维向量 $ x^{(j)} = [x_{1}^{(j)}, x_{2}^{(j)}, \\ldots, x_{n}^{(j)}], 1 \\leq j \\leq m $, 函数在这一点的变化最快的方向，就是梯度方向，方向导数沿着这个方向取得了最大值。目标函数$f(x)$有关$x$的梯度是一个由$n$个偏导数组成的向量：\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\bigg[\\frac{\\partial f(x)}{\\partial x_1^{(j)}}, \\frac{\\partial f(x)}{\\partial x_2^{(j)}}, \\ldots, \\frac{\\partial f(x)}{\\partial x_n^{(j)}}\\bigg]\n",
    "$$\n",
    "\n",
    "$f$ 沿着单位向量 $u$（$\\|u\\|=1$）方向上的变化率，即定义 $f$ 在 $x$ 上沿着$u$方向的方向导数为\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{D}_u f(x) &= \\lim_{h \\rightarrow 0}  \\frac{f(x + h u) - f(x)}{h}\\\\\n",
    "&= \\nabla f(x) \\cdot u\\\\\n",
    "&= |\\nabla f(x)| \\cdot |u| \\cdot cos(\\beta)\\\\\n",
    "&= |\\nabla f(x)| \\cdot cos(\\beta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\beta$ 为梯度 $\\nabla f(x)$ 和单位向量$u$之间的夹角，为了找到 $f$ 降低最快的方向，令$\\beta = 0$时，$cos(\\beta)$ 取得最大值 1 。\n",
    "\n",
    "损失函数对各个参数求导：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_1} = \\frac{1}{m}\\sum_{j=0}^m(\\theta_0 x_0^{(j)} + \\theta_1 x_1^{(j)} + ... + \\theta_{n}x_{n}^{(j)} - y^{(j)}) x_{1}^{(j)} \\\\\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_2} = \\frac{1}{m}\\sum_{j=0}^m(\\theta_0 x_0^{(j)} + \\theta_1 x_1^{(j)} + ... + \\theta_{n}x_{n}^{(j)} - y^{(j)}) x_{2}^{(j)} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_n} = \\frac{1}{m}\\sum_{j=0}^m(\\theta_0 x_0^{(j)} + \\theta_1 x_1^{(j)} + ... + \\theta_{n}x_{n}^{(j)} - y^{(j)}) x_{n}^{(j)}\n",
    "$$\n",
    "\n",
    "通过梯度下降算法不断降低目标函数$\\ell$的值：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla \\ell(\\theta)\\\\\n",
    "\\theta &\\leftarrow \\theta - \\eta \\bigg[\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_1}, \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_2}, \\ldots, \\frac{\\partial \\ell(\\theta)}{\\partial \\theta_n}\\bigg]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 随机梯度下降，Mini-batch 梯度下降\n",
    "计算梯度下降会使用全量数据，需要大量内存，实际中数据量大的时候会用随机梯度下降SGD（stochastic gradient descent）。\n",
    "\n",
    "随机梯度下降每次用一个数据样本进行迭代\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla \\ell^{(j)}(\\theta)\n",
    "$$\n",
    "\n",
    "每次迭代的计算开销从梯度下降的$\\mathcal{O}(m)$降到了常数$\\mathcal{O}(1)$。\n",
    "\n",
    "为了提高随机梯度下降的效率，一次选取一个小批量的样本计算梯度下降：\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla \\ell_{\\mathcal{B}_t}(\\theta)\n",
    "$$\n",
    "\n",
    "$$\\nabla \\ell_{\\mathcal{B}_t}(\\theta) =\n",
    "\\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t}\\nabla\n",
    "\\ell_i(\\theta)$$\n",
    "\n",
    "梯度下降能求到全局最优值，随机梯度下降和Mini-batch梯度下降，只能求局部最优值（可能求到全局最优值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function 用平方误差而不是绝对值误差？$^{[2]}$\n",
    "\n",
    "以真实值为中心，预测点因为随机性噪声（系统性噪声不在讨论范围）而偏离，假设偏离越远概率越小，且符合正态分布曲线（基于中心极限定理），而不是符合拉普拉斯分布。那么预测值发生的概率正比于$e^{-(H_\\theta(x) - y)^{2}}$，真实y可以理解为正态分布的期望。\n",
    "\n",
    "根据贝叶斯公式，执果索因，\n",
    "$$\n",
    "P(y|D) ∝ P(y) * P(D|y).\n",
    "$$\n",
    "<br>\n",
    "$y$ 是真实值直线，$D$ 是所有数据点。\n",
    "先验概率 $P(y)$是均匀的，因为哪条直线也不比另一条更优越。\n",
    "$P(D|y)$是真实值直线生成数据点的概率，假设各个数据点独立，<br> <br>\n",
    "$$\n",
    "\\begin{align}\n",
    "  P(D|y) &= P(D_1|y) \\cdot P(D_2|y) \\cdots P(D_n|y) \\\\\n",
    "         &= a \\cdot e^{-(H_\\theta(x)^1 - y^1)^{2}} \\cdot e^{-(H_\\theta(x)^1 - y^1)^{2}} \\cdots e^{-(H_\\theta(x)^n - y^n)^{2}} \\\\\n",
    "         &= a \\cdot e^{-\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}}\n",
    "\\end{align}\n",
    "$$\n",
    "所以最大化后验概率$P(y|D)$，就是最小化$\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}$\n",
    "\n",
    "这是基于高斯噪声分布、贝叶斯公式和最大似然估计的，对线性回归的推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯角度理解正则化$^{[3][4]}$\n",
    "\n",
    "给定观察数据$D$，$D_0, D_1, \\ldots, D_m$是样本空间$D$的一个划分，贝叶斯方法最大化后验概率，估计参数$\\theta$。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta^* &= \\operatorname*{arg\\ max}_{\\theta} p(\\theta|D) \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} \\frac{p(D|\\theta) p(\\theta)}{p(D)} \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} p(D|\\theta) p(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "$p(D)$观察数据的概率，不影响参数估计可以消去；$p(D|\\theta)$是似然函数(likelihood function)，类条件概率密度参数表达式；$p(\\theta)$是参数向量的先验概率(prior)。\n",
    "\n",
    "$$\n",
    "\\because p(D|\\theta) = \\prod\\limits_{j=0}^{m} p(D_i|\\theta)\n",
    "$$\n",
    "同时对后验概率取对数，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore \\theta^* &= \\operatorname*{arg\\ max}_{\\theta} log \\left( p(\\theta|D) \\right) \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} log \\left( \\prod\\limits_{j=0}^{m} p(D_i|\\theta) p(\\theta) \\right) \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} \\sum_{j=0}^m log p(D_i|\\theta) + log p(\\theta) \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) - log p(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "正则化一般会采用 L1 范式或者 L2 范式，来防止过拟合（目的），约束要优化的参数（本质）。\n",
    "\n",
    "从概率论的角度，\n",
    "\n",
    "Ordinary Least Square(OLS) 用Gaussian分布和最大似然估计解释；\n",
    "\n",
    "Lasso Regression 用Laplace分布和最大后验估计解释，相当于OLS加L1范式；\n",
    "\n",
    "Ridge Regression 用Gaussian分布和最大后验估计解释，相当于OLS加L2范式。\n",
    "\n",
    "Lasso Regression 给OLS模型引入了先验知识，$\\theta$服从零均值的拉普拉斯分布，\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(\\theta|\\mu,b) = \\frac{1}{2b} exp(-\\frac{|\\theta - \\mu|}{b})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta^* &= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) - log p(\\theta)\\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + log(2b) + \\frac{|\\theta - \\mu|}{b} \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + \\frac{1}{b} \\sum_{i=0}^n |\\theta_i - \\mu| \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + \\lambda \\sum_{i=0}^n |\\theta_i| \\quad\\quad (\\mu = 0, b = \\frac{1}{\\lambda})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ridge Regression 给OLS模型引入了先验知识，$\\theta$服从零均值的高斯分布，\n",
    "$$\n",
    "p(\\theta) = \\mathcal{N}(\\theta|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2})\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta^* &= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) - log p(\\theta)\\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) - log p(\\theta) \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + log(\\sqrt{2\\pi}\\sigma) + \\frac{(\\theta - \\mu)^2}{2\\sigma^2} \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + \\frac{1}{2\\sigma^2} \\sum_{i=0}^n (\\theta_i - \\mu)^2 \\\\\n",
    "&= \\operatorname*{arg\\ min}_{\\theta} -\\sum_{j=0}^m log p(D_i|\\theta) + \\lambda \\sum_{i=0}^n (\\theta_i)^2 \\quad\\quad (\\mu = 0, \\sigma = \\sqrt{\\frac{1}{2\\lambda}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L1范式、L2范式 分别是绝对值之和、平方和，都是正数，为了保持模型总体最小值，似然函数就要比不加范式的情况下要小，那么一些参数就会比不加范式小，甚至等于零。\n",
    "\n",
    "部分特征$\\theta$和最终输出$y$没有关系，在最小化目标函数时，这些特征虽然可以获得更小的训练误差，但在预测新样本时，这些特征会影响对输出的正确预测。正则化就是能实现特征的自动选择，L1范式更容易去掉多余的特征，把特征参数置为零，得到稀疏解。过拟合时，拟合函数需要顾及每一点，最终形成的拟合函数波动很大，函数值变化剧烈，也就是某些$\\theta$非常大，L2范式限制了特征参数变大的趋势，逼迫特征参数趋近于零但不为零，得到比较平滑的解，降低模型复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References：\n",
    "[1] [动手学深度学习](http://zh.d2l.ai/)\n",
    "\n",
    "[2] [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)\n",
    "\n",
    "[3] [从贝叶斯角度深入理解正则化](https://blog.csdn.net/zhuxiaodong030/article/details/54408786)\n",
    "\n",
    "[4] [【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
