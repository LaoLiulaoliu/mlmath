{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Input: $$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 & \\cdots & 3\\\\\n",
    "  4 & 4 & \\cdots & 6\\\\\n",
    "  7 & 8 & \\cdots & 9\\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  11 & 12& \\cdots & 13\\\\\n",
    "  \\end{bmatrix}_{m*n}\n",
    ",\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "  9\\\\\n",
    "  11\\\\\n",
    "  13\\\\\n",
    "  \\vdots\\\\\n",
    "  19\\\\\n",
    "  \\end{bmatrix}_{m*1}\n",
    "$$\n",
    "\n",
    "Assume $y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "And linear function fits all curves.\n",
    "\n",
    "Hypothesis: $$\n",
    "  H_\\theta(x) = \\sum\\limits_{i=0}^n \\theta_i x_i\n",
    "$$\n",
    "\n",
    "Loss Function of all samples: $$\n",
    "  L(\\theta) = \\sum\\limits_{j=0}^m \\frac{1}{2m} (H_\\theta(x)^j -\n",
    "y^j)^2\n",
    "$$\n",
    "\n",
    "Partial Derivative for every $\\theta$: $$\n",
    "  \\frac{\\partial L(\\theta)}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta(x)^j - y^j) x_i^j\n",
    "$$\n",
    "\n",
    "Gradient descent optimization of $\\theta$: $$\n",
    "  \\theta_i = \\theta_i - \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta(x)^j - y^j) x_i^j\n",
    "$$\n",
    "\n",
    "\n",
    "在实际线性回归拟合中，对于Input，m代表数据量，n代表要拟合出的方程系数，一般$m \\gg n$。\n",
    "\n",
    "当我们把线性回归拟合看作解方程组时，要求解的未知数$\\theta$有n个，就不严谨的认为一共有 n行 n个方程组成了方程组。行列式\n",
    "$\\begin{vmatrix} X \\end{vmatrix}_{n*n} \\neq 0$，根据克莱姆法则，方程组有唯一解。\n",
    "\n",
    "从矩阵的角度看，非齐次线性方程组 X$\\theta =\n",
    "Y$，系数矩阵X的秩等于增广矩阵$\\overline{X}$（即$\\begin{pmatrix}XY\\end{pmatrix}$），$r(X)=r(\\overline{X})=n$，方程组有唯一解，$\\begin{vmatrix}\n",
    "X \\end{vmatrix}_{n*n}\n",
    "$是非奇异方阵。如果$r(X)=r(\\overline{X})=r<n$，方程组有无穷多组解，就是我们说的可能优化到局部最优点。\n",
    "\n",
    "从n维空间中的向量角度看，$X_1,X_2,...,X_n$\n",
    "线性无关，且是极大线性无关组，是一组基，则向量组X的秩（也是列秩）$r(X)=n$。$r(X)<n$，则方程有很多局部最优点。极大线性无关组不唯一，有几个极大线性无关组，就有几个全局最优解。\n",
    "\n",
    "\n",
    "#### Loss function 用平方误差而不是绝对值误差？$^{[1]}$\n",
    "\n",
    "以真实值为中心，预测点因为噪音（高斯白噪声）而偏离，假设偏离越远概率越小，符合正态分布曲线（基于中心极限定理），而不是符合拉普拉斯分布。那么预测值发生的概率正比于$e^{-(H_\\theta(x) - y)^{2}}$，真实y可以理解为正态分布的期望。\n",
    "\n",
    "根据贝叶斯公式，$$\n",
    "P(y|D) ∝ P(y) * P(D|y).$$\n",
    "<br>\n",
    "y 是真实值直线，D 是所有数据点。\n",
    "先验概率 P(y)是均匀的，因为哪条直线也不比另一条更优越。\n",
    "P(D|y)是真实值直线生成数据点的概率，假设各个数据点独立，<br> <br> $$\n",
    "\\begin{align}\n",
    "  P(D|y) &= P(D_1|y) * P(D_2|y) * ... * P(D_n|y) \\\\\n",
    "         &= a * e^{-(H_\\theta(x)^1 - y^1)^{2}} * e^{-(H_\\theta(x)^1 - y^1)^{2}} * ... * e^{-(H_\\theta(x)^n - y^n)^{2}} \\\\\n",
    "         &= a * e^{-\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}}\n",
    "\\end{align}\n",
    "$$\n",
    "所以最大化后验概率 P(y|D)，就是最小化$\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}$\n",
    "\n",
    "这是基于高斯噪声分布、贝叶斯公式和最大似然估计的，对线性回归的推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广义线性模型(GLM)\n",
    "指数族分布，包括高斯分布，伯努利分布，多项式分布，泊松分布，$\\beta$分布等。\n",
    "指数分布族定义：\n",
    "$$p(y;\\eta) = b(y) \\, exp(\\eta^T T(y) - a(\\eta))$$\n",
    "* $\\eta$ 是自然参数（natural parameter）\n",
    "* T(y)是充分统计量（suﬃcient statistic）\n",
    "* $a(\\eta)$是log partition function\n",
    "\n",
    "T，a，b确定一种分布，$\\eta$是该分布的参数。一般情况下，$T(y) = y$。\n",
    "\n",
    "Bernoulli分布的指数分布族形式：$$\n",
    "\\begin{align}\n",
    "\\because p(y = 1;\\phi) &= \\phi \\\\\n",
    "p(y = 0;\\phi) &= 1 - \\phi\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore p(y;\\phi) &= \\phi^y (1-\\phi)^{1-y} \\\\\n",
    "&= exp(y log(\\phi) + (1 - y) log(1 - \\phi))\\\\\n",
    "&= exp(y log(\\frac{\\phi}{1-\\phi}) + log(1 - \\phi))\n",
    "\\end{align}\n",
    "$$\n",
    "证明$e^{ln(x)}=x$。因为$e^x$和$log_e(x)$是反函数，设$x=e^y, \\forall x > 0$，则$y=ln(x)$，所以$e^{ln(x)}=e^y=x$。\n",
    "<br /><br />\n",
    "对应参数如下：$$\n",
    "\\begin{align}\n",
    "\\eta &= log(\\frac{\\phi}{1 - \\phi}) \\Rightarrow \\phi = \\frac{1}{1 + e^{-\\eta}} \\\\\n",
    "T(y) &= y \\\\\n",
    "b(y) &= 1 \\\\\n",
    "a(\\eta) &= -log(1 - \\phi) = log(1 + e^\\eta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Gaussian分布的指数分布族形式：$$\n",
    "\\begin{align}\n",
    "p(y;\\mu) &= \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) \\\\\n",
    "&= \\frac{e^{\\frac{1}{\\sigma^2}}}{\\sqrt{2\\pi}\\sigma} exp\\left(-\\frac{(y-\\mu)^2}{2}\\right) \\\\\n",
    "&= \\frac{e^{\\sigma^{-2}}}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{1}{2} y^2) exp(\\mu y - \\frac{1}{2} \\mu^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "对应参数如下：$$\n",
    "\\begin{align}\n",
    "T(y) &= y \\\\\n",
    "\\eta &= \\mu \\\\\n",
    "a(\\eta) &= \\frac{\\mu^2}{2} = \\frac{\\eta^2}{2} \\\\\n",
    "b(y) &= \\frac{e^{\\sigma^{-2}}}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{1}{2} y^2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References：\n",
    "[1] [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
