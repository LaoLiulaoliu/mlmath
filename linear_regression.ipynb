{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Input: $$\n",
    "X =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 2 & \\cdots & 3\\\\\n",
    "  4 & 4 & \\cdots & 6\\\\\n",
    "  7 & 8 & \\cdots & 9\\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  11 & 12& \\cdots & 13\\\\\n",
    "  \\end{bmatrix}_{m*n}\n",
    ",\n",
    "Y =\n",
    "  \\begin{bmatrix}\n",
    "  9\\\\\n",
    "  11\\\\\n",
    "  13\\\\\n",
    "  \\vdots\\\\\n",
    "  19\\\\\n",
    "  \\end{bmatrix}_{m*1}\n",
    "$$\n",
    "\n",
    "Assume $y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "And linear function fits all curves.\n",
    "\n",
    "Hypothesis: $$\n",
    "  H_\\theta(x) = \\sum\\limits_{i=0}^n \\theta_i x_i\n",
    "$$\n",
    "\n",
    "Loss Function of all samples: $$\n",
    "  L(\\theta) = \\sum\\limits_{j=0}^m \\frac{1}{2m} (H_\\theta(x)^j - y^j)^2\n",
    "$$\n",
    "\n",
    "Partial Derivative for every $\\theta$: $$\n",
    "  \\frac{\\partial L(\\theta)}{\\partial \\theta_i} = \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta(x)^j - y^j) x_i^j\n",
    "$$\n",
    "\n",
    "Gradient descent optimization of $\\theta$: $$\n",
    "  \\theta_i = \\theta_i - \\frac{1}{m} \\sum\\limits_{j=0}^m (H_\\theta(x)^j - y^j) x_i^j\n",
    "$$\n",
    "\n",
    "\n",
    "在实际线性回归拟合中，对于Input，m代表数据量，n代表要拟合出的方程系数，一般$m \\gg n$。\n",
    "\n",
    "当我们把线性回归拟合看作解方程组时，要求解的未知数$\\theta$有n个，就不严谨的认为一共有 n行 n个方程组成了方程组。行列式\n",
    "$\\begin{vmatrix} X \\end{vmatrix}_{n*n} \\neq 0$，根据克莱姆法则，方程组有唯一解。\n",
    "\n",
    "从矩阵的角度看，非齐次线性方程组 X$\\theta =\n",
    "Y$，系数矩阵X的秩等于增广矩阵$\\overline{X}$（即$\\begin{pmatrix}XY\\end{pmatrix}$），$r(X)=r(\\overline{X})=n$，方程组有唯一解，$\\begin{vmatrix}\n",
    "X \\end{vmatrix}_{n*n}\n",
    "$是非奇异方阵。如果$r(X)=r(\\overline{X})=r<n$，方程组有无穷多组解，就是我们说的可能优化到局部最优点。\n",
    "\n",
    "从n维空间中的向量角度看，$X_1,X_2,...,X_n$\n",
    "线性无关，且是极大线性无关组，是一组基，则向量组X的秩（也是列秩）$r(X)=n$。$r(X)<n$，则方程有很多局部最优点。极大线性无关组不唯一，有几个极大线性无关组，就有几个全局最优解。\n",
    "\n",
    "\n",
    "#### Loss function 用平方误差而不是绝对值误差？$^{[1]}$\n",
    "\n",
    "以真实值为中心，预测点因为噪音（高斯白噪声）而偏离，假设偏离越远概率越小，符合正态分布曲线（基于中心极限定理），而不是符合拉普拉斯分布。那么预测值发生的概率正比于$e^{-(H_\\theta(x) - y)^{2}}$，真实y可以理解为正态分布的期望。\n",
    "\n",
    "根据贝叶斯公式，执果索因，$$\n",
    "P(y|D) ∝ P(y) * P(D|y).$$\n",
    "<br>\n",
    "y 是真实值直线，D 是所有数据点。\n",
    "先验概率 P(y)是均匀的，因为哪条直线也不比另一条更优越。\n",
    "$P(D|y)$是真实值直线生成数据点的概率，假设各个数据点独立，<br> <br> $$\n",
    "\\begin{align}\n",
    "  P(D|y) &= P(D_1|y) * P(D_2|y) * ... * P(D_n|y) \\\\\n",
    "         &= a * e^{-(H_\\theta(x)^1 - y^1)^{2}} * e^{-(H_\\theta(x)^1 - y^1)^{2}} * ... * e^{-(H_\\theta(x)^n - y^n)^{2}} \\\\\n",
    "         &= a * e^{-\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}}\n",
    "\\end{align}\n",
    "$$\n",
    "所以最大化后验概率$P(y|D)$，就是最小化$\\sum\\limits_{j=1}^n (H_\\theta(x)^j - y^j)^{2}$\n",
    "\n",
    "这是基于高斯噪声分布、贝叶斯公式和最大似然估计的，对线性回归的推导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression, Ridge regression\n",
    "Lasso regression 是linear regression 加入L1正则化，相当于给模型引入了先验知识，$\\theta$服从零均值的拉普拉斯分布。\n",
    "\n",
    "拉普拉斯分布：$p(\\theta) = \\mathcal{N}(\\theta|\\mu,b) = \\frac{1}{2b} exp(-\\frac{|\\theta - \\mu|}{b})$\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= P(y;\\theta,x) + P(\\theta) \\\\\n",
    "&= \\sum\\limits_{j=0}^m \\frac{1}{2m} ((\\sum\\limits_{i=0}^n \\theta_i x_i)^j - y^j)^2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ridge regression 是linear regression 加入L2正则化，相当于给模型引入了先验知识，$\\theta$服从零均值的高斯分布。\n",
    "\n",
    "高斯分布：$p(\\theta) = \\mathcal{N}(\\theta|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "给定观察数据D，贝叶斯方法最大化后验概率估计参数$\\theta$。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta^* &= \\operatorname*{arg\\ max}_{\\theta} p(\\theta|D) \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} \\frac{p(D|\\theta) p(\\theta)}{p(D)} \\\\\n",
    "&= \\operatorname*{arg\\ max}_{\\theta} p(D|\\theta) p(\\theta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References：\n",
    "[1] [数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
