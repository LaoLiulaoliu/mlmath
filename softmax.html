<h3>Softmax</h3>
<h4>Softmax的形式</h4>
<p>Sigmoid 单输出节点是确定概率值，Softmax 输出节点的输出值是概率分布。</p>
<h4>Softmax 为何加指数函数？</h4>
<ol>
<li>指数函数的曲线斜率逐渐增大，能够将输出值拉开距离。</li>
<li>指数函数在反向传播求梯度求导的时候比较方便。</li>
<li>当输出值非常大，指数值可能会溢出，所有输出值都减去max(输出值) 来优化。</li>
</ol>
<h4>Softmax 求导</h4>
<p>$$
\mathcal{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{c=1}^C e^{z_c}}
$$
i 是网络输出节点编号，$z_i$ 是网络输出节点概率，C是class分类数。</p>
<p>设 $\hat{y}<em>i = \frac{e^{z_i}}{\sum</em>{c=1}^C e^{z_c}} = p_i$ 为类i 的概率，则</p>
<ol>
<li>
<p>$i \neq j$
$$
\begin{aligned}
\frac{\partial \hat{y}<em>i}{\partial z_j} &amp;= \frac{0 - e^{z_i}
e^{z_j}}{(\sum</em>{c=1}^C e^{z_c})^2} \
&amp;= -\frac{e^{z_i}}{\sum_{c=1}^C e^{z_c}} \frac{e^{z_j}}{\sum_{c=1}^C
e^{z_c}} \
&amp;= - p_i p_j
\end{aligned}
$$</p>
</li>
<li>
<p>$i = j$
$$
\begin{aligned}
\frac{\partial \hat{y}<em>i}{\partial z_j} &amp;= \frac{e^{z_i} \sum</em>{c=1}^C e^{z_c} -
e^{z_i} e^{z_j}}{(\sum_{c=1}^C e^{z_c})^2} \
&amp;= \frac{e^{z_i} (\sum_{c=1}^C e^{z_c} - e^{z_j})}{(\sum_{c=1}^C e^{z_c})^2}
\
&amp;= \frac{e^{z_i}}{\sum_{c=1}^C e^{z_c}} \frac{\sum_{c=1}^C e^{z_c} -
e^{z_j}}{\sum_{c=1}^C e^{z_c}} \
&amp;= p_i (1 - p_j) \
&amp;= p_i - p_i^2
\end{aligned}
$$</p>
</li>
</ol>
<h4>交叉熵损失函数</h4>
<p>Softmax 是输出节点的激活函数，交叉熵是损失函数。</p>
<h6>溢出</h6>
<p>Softmax 计算输出值容易溢出，交叉熵也容易溢出，一般统一实现。</p>
<p>Softmax 分子分母同时乘以非零常数E，等式不变，
$$\hat{y}<em>i = \frac{e^{z_i}}{\sum</em>{c=1}^C e^{z_c}} = \frac{E e^{z_i}}{E
\sum_{c=1}^C e^{z_c}} = \frac{e^{z_i + log(E)}}{\sum_{c=1}^C e^{z_c +
log(E)}}
$$</p>
<p>让所有的 $z_i + log(E)$在0附近，最好小于零，所以用max没用mean，令 $log(E) = - max(z_1, z_2, ...,
z_C)$，可以有效防止溢出。</p>
<h6>损失函数</h6>
<p>一般使用最大似然估计（Maximum likelihood estimation）来构造损失函数。</p>
<p>p是模型预测的概率值，y是类标签，二分类问题：</p>
<p>$$L = P(y|x) = (p)^y (1-p)^{1-y}$$</p>
<p>多分类问题：</p>
<p>$$L = P(y|x) = \prod_{c=1}^{C} p_c^{y_c}$$</p>
<p>连乘结果容易为0，连加求导方便，用log transformation，似然函数取对数的负数，最小化对数似然函数，</p>
<p>$$\operatorname*{arg\ min} L_{CE} = - log P(y|x) = -\sum_{c=1}^{C} y_c log p_c$$</p>
<p>标准交叉熵形式中，正确类别的输出节点概率为1，同时$\sum_{c=1}^C y_c = 1$，所以上式化简为，
$$\operatorname*{arg\ min} L_{CE} = -log p_i$$</p>
<p>Softmax 取对数加负号，得到交叉熵，跟标准交叉熵形式等价。
$$- log p_i = - log(\frac{e^{z_i}}{\sum_{c=1}^C e^{z_c}})$$</p>
<h6>损失函数求导</h6>
<p>单个样本的Softmax Cross entropy 对网络输出变量 $z_j$的偏导数：
$$\begin{aligned}
\frac{\partial L_{CE}}{\partial z_j} &amp;= - \sum_{c=1}^C y_c \frac{\partial log
p_c}{\partial z_j} \
&amp;= - \sum_{c=1}^C y_c \frac{1}{p_c} \frac{\partial p_c}{\partial z_j}
\end{aligned}
$$</p>
<p>根据Softmax对 $z_j$的偏导数：
$$\frac{\partial p_i}{\partial z_j} =
\begin{cases}
- p_i p_j, &amp; \text {i $\neq$ j}\
p_i - p_i^2, &amp; \text {i = j}
\end{cases}
$$</p>
<p>两种情况相加到偏导里，
$$\begin{aligned}
\frac{\partial L_{CE}}{\partial z_j} &amp;= - \sum_{c=1}^C \frac{y_c}{p_c}
\frac{\partial p_c}{\partial z_j} \
&amp;= - \sum_{c \neq j}\frac{y_c}{p_c} * (- p_c p_j) - \frac{y_j}{p_j} (p_j -
p_j^2) \
&amp;= \sum_{c \neq j} y_c p_j - y_j + y_j p_j \
&amp;= (\sum_{c \neq j} y_c + y_j) p_j - y_j \
&amp;= p_j - y_j
\end{aligned}
$$</p>
<h4>References</h4>
<p>[1] <a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数</a></p>